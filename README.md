Desert Vision: Semantic Segmentation for Off-Road Autonomy
A submission for the Duality AI Offroad Semantic Scene Segmentation Challenge
Welcome to my hackathon project! This repository contains a deep learning pipeline designed to give Unmanned Ground Vehicles (UGVs) a fine-grained understanding of their surroundings.
By processing a synthetic dataset of desert environment twins generated by Duality AI's FalconEditor, this model learns to classify 10 distinct environmental elements at the pixel level. This highly accurate semantic segmentation is crucial for off-road obstacle avoidance and path planning.
Performance Snapshot
The model was evaluated using the Intersection over Union (IoU) metric, which measures how well the prediction overlaps with the ground truth. After training for 25 epochs, the final benchmark results are:
Metric	Final Score
Validation IoU Score	0.7171
Final Training Loss	0.2829
(Note: A steadily decreasing training loss indicates effective model training, as per the Duality AI benchmarks.)

The Technology: Custom U-Net Architecture
I chose to build a custom U-Net architecture using PyTorch for this task. U-Net is a segmentation powerhouse; its encoder-decoder structure utilizes "skip connections" to ensure the model captures both the broad context (e.g., the sky) and the sharp, fine boundaries (e.g., the exact edge of a tree).
Training Configuration:
•	Input Image Size: 256x256
•	Training Epochs: 25
•	Optimizer: Adam (Learning Rate: 1e-4)
•	Evaluation Metric: Intersection over Union (IoU)
Scene Understanding & Class Palette
The model maps raw images to beautifully color-coded masks. It has been trained to identify the following specific classes defined by the challenge dataset:
Class Index	Class Name	Output Color Guide
0	Trees	Forest Green
1	Lush Bushes	Lime Green
2	Dry Grass	Goldenrod
3	Dry Bushes	Saddle Brown
4	Ground Clutter	Gray
5	Flowers	Hot Pink
6	Logs	Dark Brown
7	Rocks	Dark Gray
8	Landscape	Sandy Brown
9	Sky	Sky Blue

How to Reproduce the Results
To ensure reproducibility, follow these instructions to run the training and inference scripts locally.
1. Prerequisites
Ensure your Python environment has the necessary dependencies installed. You will need torch, torchvision, numpy, pillow, and matplotlib.
2. File Structure
Your local data must be arranged with a dataset/ folder containing Train/, Val/, and testimages/ directories.
3. Execution Steps
•	Train the Model: Run python train.py to start the training process for the defined 25 epochs.
•	Evaluate Performance: Run python calculate_iou.py to calculate the final IoU performance score on the validation set.
•	Generate Predictions: Run python test.py to create the colorized segmentation masks for the unseen images. The outputs will be saved in the predictions folder.
Future Work & Optimizations
A hackathon project is never truly finished! To further optimize accuracy and generalizability for real-world deployment, future improvements could include:
•	Data Augmentation: Introducing random flips and rotations to the synthetic data to make the model more robust to edge cases.
•	Hyperparameter Tuning: Systematically testing dynamic learning rates to squeeze out a higher IoU score.
•	Advanced Backbones: Experimenting with replacing the standard U-Net encoder with a powerful pre-trained ResNet backbone for enhanced feature extraction.

